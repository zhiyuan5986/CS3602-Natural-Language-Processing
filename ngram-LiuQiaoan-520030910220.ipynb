{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# 作业一：实现N-gram语言模型平滑算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) * \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) * P(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现N-gram语言模型及采用带Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea6708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        # 字典的大小\n",
    "        self.vocab_size: int = vocab_size\n",
    "        \n",
    "        # 表示该语言模型n-gram的取值\n",
    "        self.n: int = n\n",
    "        \n",
    "        # 记录语料库中，某个gram的出现的频次\n",
    "        # 例如 self.frequencies[1][(12, 23)] 应等于 (12, 23) 这个 2-gram 在语料库中出现的次数\n",
    "        # 注：第一个下标表示长度，调用时下标要减一\n",
    "        self.frequencies: List[Dict[Gram, int]] = [{} for _ in range(n)]\n",
    "        \n",
    "        # 缓存语言模型提供的概率值，即对于某个gram而言 P(`gram[-1]`|`gram[:-1]`)的值\n",
    "        # 例如 self.disfrequencies[2][(12, 23, 77)] 应等于 P(`77`|`12, 23`) 的值\n",
    "        # 注：第一个下标表示长度，调用时下标要减一\n",
    "        self.disfrequencies: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "\n",
    "        # 记录出现次数为r次的N-gram种类数\n",
    "        self.N_r: Dict[int, int] = {}\n",
    "\n",
    "        # 说明文档中所提到的折扣阈值θ\n",
    "        self.discount_threshold:int = 7\n",
    "        \n",
    "        # 对于某个gram而言的$d'$\n",
    "        self._d: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "        \n",
    "        # 缓存对于某个gram而言的回退系数α\n",
    "        # 例如 self._alpha[2][(122, 323)] 应等于α(`122, 323`)\n",
    "        # 注意：调用时，第一个下标虽然表示长度，但不需要减一，因为此处的gram是上层调用时输入的某个gram的前缀。\n",
    "        self._alpha: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "        \n",
    "        # 当计算结果为0时，应取该值，避免潜在的除零错误\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        # $\\lambda = \\frac{N_1}{N_1-(\\theta+1)N_{\\theta+1}}$，用于计算$d'$\n",
    "        self.Lambda: float\n",
    "\n",
    "        # 这是__getitem__中计算长度为1的gram的 $P(gram[-1])$时用到的所有长度为1的gram出现的次数。为了避免重复计算，在`learn` 函数中缓存。\n",
    "        self.num_of_gram_with_length_1: float\n",
    "\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "\n",
    "        for stc in corpus:\n",
    "            for i in range(1, len(stc)+1):\n",
    "                for j in range(min(i, self.n)):\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    if tuple(stc[i-j-1:i]) not in self.frequencies[j].keys():\n",
    "                        self.frequencies[j][tuple(stc[i-j-1:i])] = 1\n",
    "                    else:\n",
    "                        self.frequencies[j][tuple(stc[i-j-1:i])] += 1               \n",
    "\n",
    "        for i in range(0, self.n):\n",
    "            # NOTE: calculates the value of $N_r$\n",
    "            grams = itertools.groupby(\n",
    "                sorted(map(lambda itm: (itm[0], itm[1]),self.frequencies[i].items()),key=lambda itm: itm[1]),lambda itm: itm[1]\n",
    "            )\n",
    "            \n",
    "            for k, gr in grams:\n",
    "                if k not in self.N_r:\n",
    "                    self.N_r[k] = len(list(gr))\n",
    "                else:\n",
    "                    self.N_r[k] += len(list(gr))\n",
    "\n",
    "        # NOTE: 这两个缓存的参数说明见__init__函数\n",
    "        lambda_numerator = self.N_r.get(1,self.eps)\n",
    "        lambda_denominator = (self.N_r.get(1,self.eps)-(self.discount_threshold+1)*self.N_r.get(self.discount_threshold+1,self.eps))\n",
    "        if lambda_denominator == 0:\n",
    "            self.Lambda = lambda_numerator / self.eps\n",
    "        else:\n",
    "            self.Lambda = lambda_numerator / lambda_denominator        \n",
    "        self.num_of_gram_with_length_1 = float(sum([item[1] for item in self.frequencies[0].items()]))\n",
    "\n",
    "\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the discounting factor $d'$.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        n = len(gram)-1\n",
    "        if gram not in self._d[n]:\n",
    "            # 对于某个$gram=(W_{k-n+1}^{k-1},w_k)$而言，返回$d(W_{k-n+1}^{k-1},w_k)$.\n",
    "            # TODO: calculates the value of $d$\n",
    "\n",
    "            if self.frequencies[len(gram)-1].get(gram,self.eps) > self.discount_threshold:\n",
    "                self._d[n][gram] = 1\n",
    "            else:\n",
    "                # NOTE: calculates the value of $d'$\n",
    "                # $ r = C(W_{k-n+1}^{k-1},w_k) $\n",
    "                r = self.frequencies[n].get(gram,self.eps)\n",
    "                self._d[n][gram] = self.Lambda * (r+1) * self.N_r.get(r+1,self.eps) / r / self.N_r.get(r,self.eps) +1 - self.Lambda\n",
    "\n",
    "        return self._d[n][gram]\n",
    "\n",
    "    def alpha(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n-1]:\n",
    "                # 按照定义计算分子分母的 1 - sum (...) 的值即可，\n",
    "                # 记得考虑分母为0应该怎么处理\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "\n",
    "                numerator = [gram_[1] for gram_ in self.disfrequencies[n].items() if gram == gram_[0][:-1]]\n",
    "                numerator = 1-sum(numerator)\n",
    "\n",
    "                denominator = [gram_[1] for gram_ in self.disfrequencies[n-1].items() if gram[1:] == gram_[0][:-1]]\n",
    "                denominator = 1-sum(denominator)\n",
    "\n",
    "                denominator = self.eps if denominator == 0 else denominator\n",
    "                self._alpha[n][gram] = numerator / denominator\n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)-1\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n>0:\n",
    "                # 按照P的公式，分类讨论count(gram)>0和count(gram)=0的情况\n",
    "                # TODO: calculates the smoothed probability value according to the formulae\n",
    "                if gram in self.frequencies[n]:\n",
    "                    d_ = self.d(gram)\n",
    "                    self.disfrequencies[n][gram] = d_ * self.frequencies[n][gram] / self.frequencies[n-1][gram[:-1]]\n",
    "                else:\n",
    "                    alpha_ = self.alpha(gram[:-1])\n",
    "                    self.disfrequencies[n][gram] = alpha_ * self[gram[:-1]]\n",
    "            else:\n",
    "                self.disfrequencies[n][gram] = self.frequencies[n].get(gram, self.eps) / self.num_of_gram_with_length_1\n",
    "        return self.disfrequencies[n][gram]\n",
    "        \n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        for i in range(2, len(sentence)+1):\n",
    "            # 注意这是对数运算的情况\n",
    "            # 这里i的取值会把<s>和</s>考虑进来，可以自行修改使得不考虑这两个词\n",
    "            # TODO: calculates the log probability\n",
    "            start = max(0,i-4)\n",
    "            gram = tuple(sentence[start:i])\n",
    "            log_prob += math.log(self[gram])\n",
    "        return log_prob\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "        # 和我们课件中的PPW是同一个东西，利用上面的log_prob加以计算\n",
    "        # TODO: calculates the PPL\n",
    "        return math.exp(-1/len(sentence) * self.log_prob(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "4024.4285854408345\n",
      "329.98308100051946\n",
      "115.98473767631742\n",
      "93.1752138770833\n",
      "541.1954858948097\n",
      "38.25865859400025\n",
      "13.033512752523269\n",
      "96.36248023325777\n",
      "1697.405435379303\n",
      "66.00653225865175\n",
      "59.7516412469696\n",
      "21.75448961948782\n",
      "713.4413376691816\n",
      "1893.5053083857292\n",
      "205.6865390162769\n",
      "71.58770935595584\n",
      "32893.155407496335\n",
      "19911.18192579337\n",
      "67.28307420810295\n",
      "97.65383255155868\n",
      "202.2452518656914\n",
      "1843.7025454955794\n",
      "149.72395745980316\n",
      "134.9285685522969\n",
      "95.7396117042547\n",
      "20.044531779737692\n",
      "83.38641757019148\n",
      "7.187039591498948\n",
      "66.9100171685885\n",
      "223.93527067981415\n",
      "869.3689805891278\n",
      "1631.5191645126\n",
      "42.881305194744876\n",
      "88.36889717265751\n",
      "497.28404823815663\n",
      "50.24954303479835\n",
      "391.1455808785404\n",
      "447.59020273304395\n",
      "119.7904630862471\n",
      "280.4110647283188\n",
      "75.56217850055091\n",
      "402.5829961855147\n",
      "75.56654580995954\n",
      "45.06544173377153\n",
      "383.744006022129\n",
      "355.3712430543032\n",
      "116.03356226245106\n",
      "244.56038506716578\n",
      "45.04898134658861\n",
      "98.21639408492136\n",
      "Avg:  1440.7799836910665\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\", encoding=\"utf-8\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63847b",
   "metadata": {},
   "source": [
    "<font color=\"red\">总结</font>\n",
    "\n",
    "这是本学期NLP课程的第一次作业，实现了课堂上所述的n-gram模型的一种算法。我实现算法的过程如下：\n",
    "1. 补全`learn`函数。函数需要根据数据集进行学习，这个过程分为以下几步：\n",
    "   - 计算`self.frequencies`。统计每一个gram出现的次数，根据gram长度存入`self.frequencies`中的一个字典中。\n",
    "   - 计算`self.N_r`。统计出现次数是$r$次的n-gram种类数，用于计算修正后的折扣因子$d'$。\n",
    "   - 计算`self.Lambda`。$\\lambda = \\frac{N_1}{N_1-(\\theta+1)N_{\\theta+1}}$，用于计算修正后的折扣因子。\n",
    "   - 计算`self.num_of_gram_with_length_1`。统计所有长度为1的gram出现的次数，用于计算gram长度为1时的 $P(gram[-1]|gram[:-1])$。原因在于此时的gram的前缀可看作`<s>`，需要统计所有长度为1的gram作为分母。\n",
    "\n",
    "2. 补全`d`函数。`d`函数返回修正后的折扣因子$d'$，根据算法可得，\n",
    "$$\n",
    "d(W_{k-n+1}^{k-1}, w_k) = \\begin{cases}\n",
    "    1 &  C(W_{k-n+1}^{k-1}, w_k) > \\theta \\\\\n",
    "    d'(W_{k-n+1}^{k-1}, w_k) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "3. 补全`alpha`函数。此函数返回某个`gram`的前缀`gram[:-1]`的$\\alpha$值。根据算法，\n",
    "$$\n",
    "\\alpha(W_{k-n+1}^{k-1}) = \\frac{1-\\sum_{w_k\\in V_+} P(w_k|W_{k-n+1}^{k-1})}{1-\\sum_{w_k\\in V_+} P(w_k|W_{k-n+2}^{k-1})} \n",
    "$$\n",
    "\n",
    "4. 补全`__getitem__`函数。这个函数用于计算 $P(gram[-1]|gram[:-1])$，展开条件概率公式即可计算。\n",
    "\n",
    "5. 补全`log_prob`函数。针对输入的一个sentence，计算其出现的对数概率，只需循环调用`__getitem__`函数即可\n",
    "\n",
    "6. 补全`ppl`函数，根据算法，\n",
    "$$\n",
    "PPL = \\frac{1}{\\sqrt[T]{\\prod_{t=1}^T P(x^{(t)} | x^{(t-1)}, \\dots, x^{(1)})}}\n",
    "$$\n",
    "即可得到每个句子的混淆度。注意，由于前文计算得到对数概率，此处应将对数概率求指数后再代入公式计算。\n",
    "\n",
    "<font color=\"red\"> 心得体会 </font>\n",
    "\n",
    "这次作业对数学、编程以及课堂知识的要求都比较高，在实现过程中，我也不断地考虑如何高效实现算法，并且尝试稍微修改了框架提供的参数。同时，我也和助教老师进行了交流，得到了他的帮助和指导。总体上来说，这次作业还是比较有挑战性，我也在其中锻炼了自己的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0162e",
   "metadata": {},
   "source": [
    "<font color=\"red\">附录</font>：实现字典树(前缀树) `DictTree`\n",
    "\n",
    "由于助教老师说不需要应用字典树，故在此只将字典树的实现附上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: implement a DictTree instead of using a flattern dict\n",
    "class DictTree():\n",
    "    def __init__(self):\n",
    "        self.root = {}\n",
    "        self.end = -1\n",
    "        self.len = 0\n",
    "        self.all_gram = []\n",
    "        # pass\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        self.len = 0\n",
    "        def number_of_end(node):\n",
    "            if node is not None:\n",
    "                if self.end in node:\n",
    "                    self.len += 1\n",
    "                for c in node.keys():\n",
    "                    if c != self.end:\n",
    "                        number_of_end(node[c])\n",
    "        number_of_end(self.root)\n",
    "        return self.len\n",
    "        # pass\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.all_gram = []\n",
    "        self.now_index = -1\n",
    "        def all(node, word):\n",
    "            if node is not None:\n",
    "                # print(node)\n",
    "                if self.end in node.keys():\n",
    "                    self.all_gram.append(tuple(word))\n",
    "                for c in node.keys():\n",
    "                    if c != self.end:\n",
    "                        all(node[c],word+[c])\n",
    "        word = [] \n",
    "        all(self.root,word)\n",
    "        return self\n",
    "        # pass\n",
    "    def __next__(self):\n",
    "        self.now_index += 1\n",
    "        if self.now_index < len(self.all_gram):\n",
    "            return self.all_gram[self.now_index]\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "\n",
    "    def __contains__(self, key: Gram):\n",
    "        curNode = self.root\n",
    "        for c in key:\n",
    "            if not c in curNode:\n",
    "                return False\n",
    "            curNode = curNode[c]\n",
    "        if not self.end in curNode:\n",
    "            return False\n",
    "        return True\n",
    "        # pass\n",
    "\n",
    "    def __getitem__(self, key: Gram) -> int:\n",
    "        curNode = self.root\n",
    "        # print(key)\n",
    "        for c in key:\n",
    "            if c not in curNode:\n",
    "                return -1\n",
    "            curNode = curNode[c]\n",
    "        if not self.end in curNode:\n",
    "            return -1\n",
    "        return curNode[self.end]\n",
    "        # pass\n",
    "\n",
    "    def __setitem__(self, key: Gram, frequency: int):\n",
    "        curNode = self.root\n",
    "        for c in key:\n",
    "            if c not in curNode:\n",
    "                curNode[c] = {}\n",
    "            curNode = curNode[c]\n",
    "        curNode[self.end] = frequency\n",
    "        # pass\n",
    "\n",
    "    def __delitem__(self, key: Gram):\n",
    "        curNode = self.root\n",
    "        for c in key:\n",
    "            curNode = curNode[c]\n",
    "        del curNode[self.end]\n",
    "        # pass\n",
    "    def get(self, key: Gram, other):\n",
    "        if self.__contains__(key):\n",
    "            return self.__getitem__(key)\n",
    "        else:\n",
    "            return other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
